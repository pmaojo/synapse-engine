#!/usr/bin/env python3
"""
Demo: Sistema de Embeddings y Fine-Tuning
Muestra c√≥mo se usan embeddings para b√∫squeda sem√°ntica
y c√≥mo entrenar un modelo personalizado
"""
import sys
import os
sys.path.append(os.getcwd())

from agents.infrastructure.persistence.embeddings import EmbeddingGenerator
from agents.infrastructure.persistence.vector_store import VectorStore
import numpy as np

def demo_embeddings():
    print("="*70)
    print("DEMO: EMBEDDINGS Y B√öSQUEDA SEM√ÅNTICA")
    print("="*70)
    
    # Inicializar
    print("\nüì¶ Inicializando componentes...")
    embedder = EmbeddingGenerator()
    vector_store = VectorStore(collection_name="demo_embeddings", dimension=384)
    
    print(f"‚úì Modelo: {embedder.model_name}")
    print(f"‚úì Dimensi√≥n: {embedder.dimension}D")
    print(f"‚úì Vector Store: Qdrant (local)\n")
    
    # Datos de ejemplo del dominio agr√≠cola
    concepts = [
        ("compost", "Material org√°nico descompuesto que mejora el suelo"),
        ("swale", "Zanja a nivel que captura agua de escorrent√≠a"),
        ("guild", "Grupo de plantas que se benefician mutuamente"),
        ("mulch", "Capa protectora sobre el suelo"),
        ("nitrogen_fixer", "Planta que fija nitr√≥geno atmosf√©rico"),
    ]
    
    print("üå± PASO 1: INDEXAR CONCEPTOS DEL DOMINIO")
    print("-"*70)
    
    for concept_id, description in concepts:
        # Generar embedding
        embedding = embedder.encode_single(description)
        
        # Guardar en vector store
        vector_store.add(
            node_id=concept_id,
            vector=embedding,
            metadata={"description": description}
        )
        print(f"  ‚úì {concept_id}: {embedding[:3]}... (384D)")
    
    print(f"\nüìä Total indexado: {len(concepts)} conceptos\n")
    
    # B√∫squeda sem√°ntica
    print("üîç PASO 2: B√öSQUEDA SEM√ÅNTICA")
    print("-"*70)
    
    queries = [
        "¬øQu√© mejora la fertilidad del suelo?",
        "T√©cnicas para conservar agua",
        "Plantas que enriquecen el suelo"
    ]
    
    for query in queries:
        print(f"\nQuery: '{query}'")
        query_embedding = embedder.encode_single(query)
        results = vector_store.search(query_embedding, top_k=2)
        
        print("  Resultados:")
        for i, result in enumerate(results, 1):
            print(f"    {i}. {result.node_id} (score: {result.score:.3f})")
            print(f"       {result.metadata['description']}")
    
    print("\n" + "="*70)
    print("üí° FINE-TUNING DE EMBEDDINGS")
    print("="*70)
    print("""
Para entrenar embeddings espec√≠ficos del dominio:

1. PREPARAR DATOS DE ENTRENAMIENTO:
   - Pares positivos: (texto, concepto) relacionados
   - Pares negativos: (texto, concepto) NO relacionados
   
   Ejemplo:
   + ("El compost mejora el suelo", "compost")
   + ("Swales capturan agua", "swale")
   - ("El compost mejora el suelo", "nitrogen_fixer")

2. USAR SENTENCE-TRANSFORMERS:
   ```python
   from sentence_transformers import SentenceTransformer, losses
   from torch.utils.data import DataLoader
   
   model = SentenceTransformer('all-MiniLM-L6-v2')
   train_dataloader = DataLoader(train_data, batch_size=16)
   
   train_loss = losses.CosineSimilarityLoss(model)
   model.fit(
       train_objectives=[(train_dataloader, train_loss)],
       epochs=10
   )
   ```

3. BENEFICIOS:
   - Embeddings alineados con tu ontolog√≠a
   - Mejor precisi√≥n en b√∫squeda sem√°ntica
   - Captura relaciones espec√≠ficas del dominio

4. M√âTRICAS:
   - Precision@K: % de resultados relevantes en top-K
   - MRR (Mean Reciprocal Rank): Posici√≥n del primer relevante
   - nDCG: Calidad del ranking completo
""")
    
    print("="*70)
    print("ARQUITECTURA ACTUAL")
    print("="*70)
    print(f"""
Modelo Base:     {embedder.model_name}
Dimensi√≥n:       {embedder.dimension}D
Vector Store:    Qdrant (persistente)
Uso:             RAG, b√∫squeda sem√°ntica, clustering
Fine-Tuning:     Posible con datos del dominio
""")

if __name__ == "__main__":
    demo_embeddings()
