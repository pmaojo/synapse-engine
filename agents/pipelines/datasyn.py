"""
Document Processing Pipeline
Handles large files with chunking and context-aware processing
"""
import csv
import json
from typing import List, Dict, Any, Iterator, Tuple
from pathlib import Path
from agents.application.pipelines.engine import PipelineStrategy, PipelineResult
from agents.infrastructure.ai.air import get_air, RewardSignal

class DocumentProcessor:
    """Chunks documents intelligently to avoid token limits"""
    
    MAX_CHUNK_SIZE = 500  # characters per chunk
    OVERLAP = 50  # overlap between chunks for context
    
    @staticmethod
    def chunk_text(text: str, max_size: int = MAX_CHUNK_SIZE, overlap: int = OVERLAP) -> Iterator[str]:
        """Split text into overlapping chunks"""
        if len(text) <= max_size:
            yield text
            return
        
        start = 0
        while start < len(text):
            end = start + max_size
            
            # Try to break at sentence boundary
            if end < len(text):
                # Look for period, newline, or semicolon
                for char in ['. ', '\n', '; ']:
                    last_break = text[start:end].rfind(char)
                    if last_break > max_size // 2:  # Don't break too early
                        end = start + last_break + len(char)
                        break
            
            yield text[start:end].strip()
            start = end - overlap
    
    @staticmethod
    def read_csv_rows(filepath: Path, max_rows: int = 100) -> Iterator[Dict[str, str]]:
        """Read CSV in batches"""
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            batch = []
            for i, row in enumerate(reader):
                batch.append(row)
                if len(batch) >= max_rows:
                    yield batch
                    batch = []
            if batch:
                yield batch

class DataSynPipeline(PipelineStrategy):
    """
    Process DataSyn files: CSVs, markdown, JSON
    Chunks large files and extracts triples incrementally
    """
    
    @property
    def name(self) -> str:
        return "DataSyn Processor"
    
    def run(self, input_data: str, **kwargs) -> PipelineResult:
        """
        Args:
            input_data: Filename or path to process
            kwargs: Additional arguments, e.g., namespace
        """
        import time
        from agents.grpc.client import get_client
        
        # Extract namespace from kwargs
        namespace = kwargs.get("namespace", "default")

        # Initialize AIR for this run
        air = get_air()
        air.reset()
        
        start_time = time.time()
        logs = []
        all_triples = []
        
        filepath = Path(input_data)
        if not filepath.exists():
            filepath = Path("documents/DataSyn") / input_data
        
        if not filepath.exists():
            return PipelineResult(
                success=False,
                data={"error": f"File not found: {input_data}"},
                logs=[f"‚ùå File not found: {input_data}"],
                execution_time=time.time() - start_time
            )
        
        logs.append(f"üìÑ Processing: {filepath.name}")
        logs.append(f"üìè Size: {filepath.stat().st_size / 1024:.1f} KB")
        logs.append(f"üë§ Tenant: {namespace}")
        
        # AIR: Reward for successful file access
        air.record_event(RewardSignal.CSV_PARSED, {"file": filepath.name})
        
        # Route by file type
        if filepath.suffix == '.csv':
            triples = self._process_csv(filepath, logs, namespace=namespace)
        elif filepath.suffix == '.md':
            triples = self._process_markdown(filepath, logs)
        elif filepath.suffix == '.json':
            triples = self._process_json(filepath, logs)
        else:
            logs.append(f"‚ö†Ô∏è Unsupported file type: {filepath.suffix}")
            triples = []
        
        all_triples.extend(triples)
        
        # AIR: Reward for triple extraction
        if triples:
            air.record_event(RewardSignal.TRIPLE_EXTRACTED, {"count": len(triples)})
        
        # Store in Rust backend
        rust_client = get_client()
        if all_triples and rust_client.connected:
            storage_result = rust_client.ingest_triples(all_triples, namespace=namespace)
            logs.append(f"üíæ Rust Storage: {storage_result.get('edges_added', 0)} edges, {storage_result.get('nodes_added', 0)} nodes")
            
            # AIR: Reward for successful storage
            air.record_event(RewardSignal.RUST_STORED, {"edges": storage_result.get('edges_added', 0)})
            
            # Update global graph state
            import sys
            if 'app' in sys.modules:
                # Access the global stored_triples from app.py
                try:
                    import app
                    app.stored_triples.extend(all_triples)
                    logs.append("üìä Updated live graph")
                except:
                    pass
        
        # Add AIR summary to logs
        logs.append("\nüìä AIR Rewards:")
        logs.append(air.get_summary())
        
        return PipelineResult(
            success=True,
            data={
                "file": filepath.name,
                "triples_extracted": len(all_triples),
                "triples": [f"({s}, {p}, {o})" for s, p, o in all_triples[:10]],  # Show first 10
                "total_chunks_processed": len(logs) - 2
            },
            logs=logs,
            execution_time=time.time() - start_time
        )
    
    
    
    def _process_csv(self, filepath: Path, logs: List[str], namespace: str = "default") -> List[Tuple[str, str, str]]:
        """
        Optimized Incremental CSV Processing + OWL Reasoning
        Based on: iText2KG (2024), GraphRAG (2024)
        
        Pipeline:
        1. Batch index all rows (efficiency)
        2. Per-row: RAG lookup ‚Üí Extract ‚Üí Validate ‚Üí Store
        3. Every 100 rows: Trigger OWL reasoning
        """
        from agents.storage.embeddings import EmbeddingGenerator
        from agents.storage.vector_store import VectorStore
        from agents.grpc.client import get_client
        from agents.domain.services.ontology import OntologyService
        from agents.validation.ontology_validator import OntologyValidator
        
        triples = []
        embedder = EmbeddingGenerator()
        vector_store = VectorStore(collection_name=f"csv_{filepath.stem}", dimension=384, namespace=namespace)
        rust_client = get_client()
        
        # Initialize Validator
        try:
            ontology = OntologyService(["ontology/core.owl", "ontology/agriculture.owl"])
            validator = OntologyValidator(ontology)
            logs.append("üõ°Ô∏è Ontology Validator Active")
        except Exception as e:
            logs.append(f"‚ö†Ô∏è Validator init failed: {e}")
            validator = None

        logs.append(f"üîç Optimized Incremental + OWL: {filepath.name}")
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                header = reader.fieldnames
                
                if not header:
                    logs.append("‚ö†Ô∏è Empty CSV")
                    return []
                
                logs.append(f"üìã Columns: {', '.join(header)}")
                
                for row_num, row in enumerate(reader, 1):
                    # RAG lookup
                    row_text = " | ".join([f"{k}: {v}" for k, v in row.items() if v])
                    query_emb = embedder.encode_single(row_text)
                    similar = vector_store.search(query_emb, top_k=3, namespace=namespace)
                    
                    # Extract with context
                    rag_context = [doc.metadata.get("description", "") for doc in similar]
                    row_triples = self._extract_optimized(row, rag_context, header)
                    
                    # Validation Step
                    if validator and row_triples:
                        val_result = validator.validate_batch(row_triples)
                        valid_triples = val_result["valid_triples"]

                        if len(valid_triples) < len(row_triples):
                            logs.append(f"  ‚ö†Ô∏è Rejected {len(row_triples) - len(valid_triples)} invalid triples")
                            for invalid in val_result["invalid_triples"]:
                                logs.append(f"    - Invalid: {invalid}")

                        row_triples = valid_triples

                    # Store immediately
                    if row_triples and rust_client.connected:
                        rust_client.ingest_triples(row_triples, namespace=namespace)
                        triples.extend(row_triples)
                    
                    # Index for future RAG
                    vector_store.add(
                        node_id=f"{filepath.stem}_row_{row_num}",
                        embedding=query_emb,
                        metadata={"description": row_text[:200]},
                        namespace=namespace
                    )
                    
                    # OWL reasoning every 100 rows
                    if row_num % 100 == 0:
                        logs.append(f"  üß† OWL reasoning at row {row_num}...")
                        inferred = self._owl_reasoning(triples[-100:])
                        if inferred:
                            rust_client.ingest_triples(inferred, namespace=namespace)
                            triples.extend(inferred)
                            logs.append(f"    +{len(inferred)} inferred triples")
                        logs.append(f"  ‚úì {row_num} rows, {len(triples)} total triples")
                    
                    if row_num >= 1000:
                        logs.append(f"‚ö†Ô∏è Limit reached at {row_num} rows")
                        break
                
                # Final OWL pass
                logs.append("üß† Final OWL reasoning...")
                final_inferred = self._owl_reasoning(triples[-100:])
                if final_inferred:
                    rust_client.ingest_triples(final_inferred, namespace=namespace)
                    triples.extend(final_inferred)
                
                logs.append(f"‚úÖ Complete: {len(triples)} triples")
                return triples
                
        except Exception as e:
            logs.append(f"‚ùå Failed: {e}")
            return []
    
    def _extract_optimized(self, row: Dict, rag_context: List[str], header: List[str]) -> List[Tuple[str, str, str]]:
        """Optimized extraction with RAG context"""
        triples = []
        
        # Main entity
        main_entity = next((v.strip() for v in row.values() if v and v.strip()), None)
        if not main_entity:
            return []
        
        # Extract with context-aware predicates
        for col_name, col_value in row.items():
            if not col_value or col_value == main_entity:
                continue
            
            predicate = self._smart_predicate(col_name, rag_context)
            triples.append((main_entity, predicate, col_value.strip()))
        
        return triples
    
    def _smart_predicate(self, col_name: str, rag_context: List[str]) -> str:
        """Infer predicate from column + RAG context"""
        col_lower = col_name.lower()
        
        if "family" in col_lower or "familia" in col_lower:
            return "belongsTo"
        elif "type" in col_lower or "tipo" in col_lower:
            return "isA"
        elif "height" in col_lower or "altura" in col_lower:
            return "hasHeight"
        else:
            return col_name.replace(" ", "_").replace("-", "_")
    
    def _owl_reasoning(self, recent_triples: List[Tuple]) -> List[Tuple[str, str, str]]:
        """Apply OWL reasoning to recent triples"""
        try:
            from agents.core.ontology import OntologyService
            from agents.tools.owl_reasoner import OWLReasoningAgent
            
            ontology = OntologyService(["ontology/core.owl", "ontology/agriculture.owl"])
            reasoner = OWLReasoningAgent(ontology.graph)
            
            result = reasoner.infer(recent_triples)
            return result.get("inferred_triples", [])
        except:
            return []
    
    def _extract_with_rag_context(self, row: Dict[str, str], context: Dict) -> List[Tuple[str, str, str]]:
        """
        Multi-stage extraction pipeline:
        
        STAGE 1: RAG to similar rows ‚Üí Extract initial triples
        STAGE 2: LLM with Cypher ‚Üí Validate/enrich triples
        STAGE 3: RAG to literature ‚Üí Extract additional triples
        
        Args:
            row: Current CSV row
            context: RAG context with similar rows
        """
        triples = []
        
        # Get main entity (first non-empty value)
        main_entity = None
        for value in row.values():
            if value and value.strip():
                main_entity = value.strip()
                break
        
        if not main_entity:
            return []
        
        # LAYER 1: RAG to similar CSV rows (already in context)
        similar_context = "\n".join(context.get("similar_rows", [])[:2])
        
        # LAYER 2: RAG to ingestion documents
        document_context = self._get_document_context(main_entity)
        
        # LAYER 3: SLM extraction with full context
        row_text = " | ".join([f"{k}: {v}" for k, v in row.items() if v])
        
        # Build rich prompt for SLM
        extraction_prompt = f"""Extract semantic triples from this CSV row.

CSV Row: {row_text}

Context from similar rows:
{similar_context}

Context from knowledge base:
{document_context}

Extract triples in format: (subject, predicate, object)
Focus on: taxonomic relationships, properties, ecological interactions.
"""
        
        # Use SLM for extraction (or fallback to rules)
        slm_triples = self._extract_with_slm(extraction_prompt, row, context)
        
        if slm_triples:
            triples.extend(slm_triples)
        else:
            # Fallback to rule-based extraction
            triples.extend(self._extract_with_rules(row, context))
        
        return triples
    
    def _get_document_context(self, entity: str) -> str:
        """
        LAYER 3: RAG search in ingestion documents.
        Search in manual_permacultura, especies.md, etc.
        """
        from agents.storage.embeddings import EmbeddingGenerator
        from agents.storage.vector_store import VectorStore
        
        try:
            # Use global vector store with ingestion docs
            embedder = EmbeddingGenerator()
            doc_store = VectorStore(collection_name="grafoso_demo", dimension=384)
            
            # Search for entity in documents
            query_emb = embedder.encode_single(entity)
            results = doc_store.search(query_emb, top_k=2)
            
            if results:
                context = "\n".join([r.metadata.get("description", "") for r in results])
                return context[:300]  # Limit context size
            
        except Exception as e:
            pass
        
        return "No additional context found"
    
    def _extract_with_slm(self, prompt: str, row: Dict, context: Dict) -> List[Tuple[str, str, str]]:
        """
        Use SLM (Small Language Model) for extraction.
        Falls back to rules if SLM not available.
        """
        try:
            from agents.infrastructure.di_container import get_container
            container = get_container()
            slm = container.slm()

            # Generate with SLM
            response = slm.generate(prompt, max_new_tokens=128)

            # Parse response (expecting JSON list of triples)
            import re
            import json

            triples = []

            # Try to find JSON array in response
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                try:
                    data = json.loads(json_match.group(0))
                    for item in data:
                        if isinstance(item, list) and len(item) == 3:
                            triples.append(tuple(item))
                except:
                    pass

            # Fallback: regex for (s, p, o) format
            if not triples:
                pattern = r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
                matches = re.findall(pattern, response)
                for match in matches:
                    triples.append(match)

            return triples

        except Exception as e:
            # print(f"SLM extraction failed: {e}")
            return []  # Will use fallback
    
    def _extract_with_rules(self, row: Dict[str, str], context: Dict) -> List[Tuple[str, str, str]]:
        """
        Enhanced rule-based extraction using context.
        """
        triples = []
        
        # Get main entity
        main_entity = None
        for value in row.values():
            if value and value.strip():
                main_entity = value.strip()
                break
        
        if not main_entity:
            return []
        
        # Extract relationships with context-aware predicates
        header = context.get("header", [])
        for col_name, col_value in row.items():
            if not col_value or col_value == main_entity:
                continue
            
            # Use RAG context to infer better predicates
            predicate = self._infer_predicate_from_context(
                col_name, 
                col_value, 
                context.get("similar_rows", [])
            )
            
            triples.append((main_entity, predicate, col_value.strip()))
        
        return triples
    
    def _infer_predicate_from_context(self, col_name: str, col_value: str, similar_rows: List[str]) -> str:
        """Infer predicate using RAG context"""
        # Check if similar rows mention this column
        col_lower = col_name.lower()
        
        if "family" in col_lower or "familia" in col_lower:
            return "belongsTo"
        elif "type" in col_lower or "tipo" in col_lower:
            return "isA"
        elif "height" in col_lower or "altura" in col_lower:
            return "hasHeight"
        else:
            # Default: use column name
            return col_name.replace(" ", "_").replace("-", "_")

    
    
    def _process_markdown(self, filepath: Path, logs: List[str]) -> List[tuple]:
        """Process markdown with chunking"""
        triples = []
        processor = DocumentProcessor()
        
        text = filepath.read_text(encoding='utf-8')
        logs.append(f"üìù Text length: {len(text)} chars")
        
        chunks = list(processor.chunk_text(text, max_size=400))
        logs.append(f"üî™ Split into {len(chunks)} chunks")
        
        for i, chunk in enumerate(chunks, 1):
            if i % 10 == 0:
                logs.append(f"  Processing chunk {i}/{len(chunks)}...")
            
            chunk_triples = self._extract_from_text(chunk)
            triples.extend(chunk_triples)
        
        logs.append(f"‚úÖ Extracted {len(triples)} triples from markdown")
        return triples
    
    def _process_json(self, filepath: Path, logs: List[str]) -> List[tuple]:
        """Process JSON"""
        triples = []
        
        data = json.loads(filepath.read_text(encoding='utf-8'))
        logs.append(f"üì¶ JSON structure: {type(data).__name__}")
        
        # Simple extraction from JSON keys
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, str) and len(value) < 100:
                    triples.append((key, "hasValue", value))
        
        logs.append(f"‚úÖ Extracted {len(triples)} triples from JSON")
        return triples
    
    def _extract_from_row(self, row: Dict[str, str], header: List[str]) -> List[tuple]:
        """
        Extract triples from a CSV row.
        Header is included for context (like a prompt template).
        
        In a real system, this would call:
        LLM/SLM with prompt: "Given CSV with columns: {header}, extract triples from: {row}"
        """
        triples = []
        
        # Example: if row has 'species' and 'family'
        if 'species' in row and 'family' in row:
            species = row['species'].strip()
            family = row['family'].strip()
            if species and family:
                triples.append((species, "belongsTo", family))
        
        # Use header to understand column semantics
        # First column is typically the subject/entity
        if header and len(header) >= 2:
            subject_col = header[0]
            subject = row.get(subject_col, '').strip()
            
            if subject:
                # Extract properties from other columns
                for col in header[1:4]:  # Limit to 3 properties per row
                    value = row.get(col, '').strip()
                    if value and len(value) < 50:  # Skip long values
                        # Use column name as predicate
                        predicate = col.replace('_', ' ').replace('-', ' ')
                        triples.append((subject, predicate, value))
        
        return triples
    
    def _extract_from_text(self, text: str) -> List[tuple]:
        """Extract triples from text chunk (simple rule-based)"""
        triples = []
        text_lower = text.lower()
        
        # Simple pattern matching
        if "plant" in text_lower or "especie" in text_lower:
            if "familia" in text_lower or "family" in text_lower:
                triples.append(("Plant", "hasProperty", "Family"))
        
        if "suelo" in text_lower or "soil" in text_lower:
            if "nutriente" in text_lower or "nutrient" in text_lower:
                triples.append(("Soil", "contains", "Nutrients"))
        
        # More sophisticated extraction would use the SLM here
        # For now, keeping it simple to avoid token limits
        
        return triples
